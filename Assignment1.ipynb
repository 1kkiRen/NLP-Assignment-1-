{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering WORDS: 100%|██████████| 1104935/1104935 [00:00<00:00, 5227209.28it/s]\n",
      "Filtering PAIRS: 100%|██████████| 24280903/24280903 [00:10<00:00, 2262028.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def pairs(text):\n",
    "    return zip(text, text[1:])\n",
    "\n",
    "# Read the file once.\n",
    "with open('./text.txt', encoding='utf-8') as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "word_list = words(text_content)\n",
    "WORDS = Counter(word_list)\n",
    "PAIRS = Counter(pairs(word_list))\n",
    "\n",
    "# Filter out words with counts <= 150.\n",
    "WORDS = Counter({\n",
    "    word: count \n",
    "    for word, count in tqdm(WORDS.items(), desc=\"Filtering WORDS\")\n",
    "    if count > 150\n",
    "})\n",
    "\n",
    "# Filter PAIRS such that both words in the pair exist in the filtered WORDS.\n",
    "PAIRS = Counter({\n",
    "    pair: count \n",
    "    for pair, count in tqdm(PAIRS.items(), desc=\"Filtering PAIRS\")\n",
    "    if pair[0] in WORDS and pair[1] in WORDS\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_SMOOTHING_VALUE = 1 / (sum(WORDS.values()) + 1)\n",
    "PAIRS_SMOOTHING_VALUE = 1 / (sum(PAIRS.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: this is a simple sentence\n"
     ]
    }
   ],
   "source": [
    "class SpellCorrector:\n",
    "\n",
    "    def P(self, word):\n",
    "        \"Probability of `word`.\"\n",
    "        return WORDS[word] / sum(WORDS.values()) if WORDS[word] > 0 else WORDS_SMOOTHING_VALUE\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"Generate possible spelling corrections for `word`.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"Create a set of possible edits that are one edit away from `word`.\"\n",
    "        letters     = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits      = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes     = [L + R[1:] for L, R in splits if R]\n",
    "        transposes  = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces    = [L + c + R[1:] for L, R in splits for c in letters if R]\n",
    "        inserts     = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"Create a set of possible edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
    "\n",
    "    def bigram_prob(self, w1, w2):\n",
    "        \"Return the probability of w2 following w1, using BIGRAMS counts and WORDS.\"\n",
    "        if w1 is None:\n",
    "            return 1\n",
    "        \n",
    "        return PAIRS[w1, w2] / WORDS[w1] if PAIRS[w1, w2] > 0 else PAIRS_SMOOTHING_VALUE\n",
    "\n",
    "    def correction(self, sentence):\n",
    "        \"\"\"\n",
    "        Perform global, bidirectional context-sensitive correction.\n",
    "        Computes forward (left-to-right) and backward (right-to-left) DP tables\n",
    "        using bigram probabilities, then combines them to choose the best candidate.\n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = sentence.split()\n",
    "        n = len(tokens)\n",
    "        \n",
    "        # For each token, get candidate corrections.\n",
    "        candidates_list = [([word] if word in WORDS else list(self.candidates(word))) for word in tokens]\n",
    "        \n",
    "        # Forward DP pass.\n",
    "        dp_forward = [{} for _ in range(n)]\n",
    "        # Initialize first token.\n",
    "        for cand in candidates_list[0]:\n",
    "            dp_forward[0][cand] = (self.P(cand), None)  # (score, backpointer)\n",
    "        \n",
    "        # Fill forward table.\n",
    "        for i in range(1, n):\n",
    "            for cand in candidates_list[i]:\n",
    "                best_prev = None\n",
    "                best_score = 0\n",
    "                for prev_cand, (prev_score, _) in dp_forward[i - 1].items():\n",
    "                    score = prev_score * self.bigram_prob(prev_cand, cand)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev = prev_cand\n",
    "                dp_forward[i][cand] = (best_score * self.P(cand), best_prev)\n",
    "        \n",
    "        # Backward DP pass.\n",
    "        dp_backward = [{} for _ in range(n)]\n",
    "        # Initialize last token.\n",
    "        for cand in candidates_list[-1]:\n",
    "            dp_backward[-1][cand] = (self.P(cand), None)\n",
    "        \n",
    "        # Fill backward table.\n",
    "        for i in range(n - 2, -1, -1):\n",
    "            for cand in candidates_list[i]:\n",
    "                best_next = None\n",
    "                best_score = 0\n",
    "                for next_cand, (next_score, _) in dp_backward[i + 1].items():\n",
    "                    score = self.bigram_prob(cand, next_cand) * next_score\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_next = next_cand\n",
    "                dp_backward[i][cand] = (self.P(cand) * best_score, best_next)\n",
    "        \n",
    "        # Combine forward and backward scores.\n",
    "        # For each position choose the candidate that maximizes the product\n",
    "        # of forward and backward scores.\n",
    "        best_sequence = []\n",
    "        for i in range(n):\n",
    "            best_cand = max(\n",
    "                candidates_list[i],\n",
    "                key=lambda cand: dp_forward[i][cand][0] *\n",
    "                                dp_backward[i][cand][0]\n",
    "            )\n",
    "            best_sequence.append(best_cand)\n",
    "        \n",
    "        return \" \".join(best_sequence)\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"ths is a smple sentnce\"\n",
    "correction = SpellCorrector().correction\n",
    "corrected = correction(sentence)\n",
    "print(\"Corrected Sentence:\", corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification of the approach\n",
    "\n",
    "#### Overview:\n",
    "The implemented spell checker uses a context-sensitive approach based on a bidirectional N-gram language model. This method leverages both left and right contexts to improve the accuracy of spelling corrections, especially in sentences where the context significantly influences the correct word choice.\n",
    "\n",
    "#### Key Components and Decisions:\n",
    "\n",
    "1. **N-gram Language Model**:\n",
    "   - **Bigram Model**: The model uses bigrams to capture the relationship between consecutive words. This helps in understanding the context better than a unigram model, which considers words independently.\n",
    "   - **Bidirectional Correction**: The correction algorithm performs two passes—one from left to right (forward) and one from right to left (backward). This bidirectional approach ensures that the context from both sides of a word is considered, leading to more accurate corrections.\n",
    "\n",
    "2. **Probability Calculations**:\n",
    "   - **Unigram Probability**: The probability of a word occurring independently is calculated based on its frequency in the corpus.\n",
    "   - **Bigram Probability**: The probability of a word given its preceding word is calculated using bigram counts. This helps in determining the likelihood of word sequences.\n",
    "   - **Smoothing**: Smoothing is applied to handle unseen bigrams, ensuring that the model can still make reasonable predictions even for rare or unseen word pairs.\n",
    "\n",
    "3. **Dynamic Programming (Viterbi Algorithm)**:\n",
    "   - **Forward Pass**: In the forward pass, the algorithm computes the best sequence of corrections from the beginning to the end of the sentence.\n",
    "   - **Backward Pass**: In the backward pass, the algorithm computes the best sequence of corrections from the end to the beginning of the sentence.\n",
    "   - **Combination of Scores**: The final correction is determined by combining the scores from both the forward and backward passes, selecting the candidate that maximizes the combined score.\n",
    "\n",
    "4. **Candidate Generation**:\n",
    "   - **Known Words**: The algorithm first checks if the word is already known (exists in the vocabulary).\n",
    "   - **One-Edit Distance**: If the word is not known, the algorithm generates candidates that are one edit away (insertions, deletions, substitutions, and transpositions).\n",
    "   - **Two-Edit Distance**: If no known candidates are found within one edit distance, the algorithm generates candidates that are two edits away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "\n",
    "\n",
    "#### The dataset:\n",
    "\n",
    "For the assignment large unified text corpus was used. The texts that were used to create the corpus are:\n",
    "- [eng_news_2015_1M-sentences.txt](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_news_2018_1M-sentences.txt](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_news_2019_1M-sentences.txt](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_news_2023_1M-sentences.txt](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_news_2024_1M-sentences.txt](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_wikipedia_2010_1M-sentences](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_wikipedia_2012_1M-sentences](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng_wikipedia_2016_1M-sentences](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "- [eng-simple_wikipedia_2021_300K-sentences](https://wortschatz.uni-leipzig.de/en/download/English)\n",
    "\n",
    "#### Probabilities:\n",
    "- If the word is unknown, the probability of its correction is $\\frac{1}{|V|}$, where $|V|$ is the total number of words in the vocabulary.\n",
    "- If the word is known, the probability of its independent correction is $P(w) = \\frac{C(w)}{|V|}$, where $C(w)$ is the count of the word in the corpus and $|V|$ is the total number of words in the vocabulary.\n",
    "- If the word is known, the probability of its left-dependent correction is $P(w|w_{i-1}) = P(w) * \\frac{C(w_{i-1}, w)}{C(w_{i-1})}$, where $C(w, w_{i-1})$ is the count of the bigram $(w_{i-1}, w)$ in the corpus and $C(w_{i-1})$ is the count of the word $w_{i-1}$ in the corpus.\n",
    "- If the word is known, the probability of its right-dependent correction is $P(w|w_{i+1}) = P(w) * \\frac{C(w, w_{i+1})}{C(w_{i+1})}$, where $C(w, w_{i+1})$ is the count of the bigram $(w, w_{i+1})$ in the corpus and $C(w_{i+1})$ is the count of the word $w_{i+1}$ in the corpus.\n",
    "- If the bigram is unknown, the probability of its correction is $\\frac{1}{|B|}$, where $|B|$ is the total number of bigrams.\n",
    "- The bidirectional probability of the bigram is $P(w_{i-1}, w_{i+1}|w) = P(w|w_{i-1}) * P(w|w_{i+1})$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorvigSpellCorrector:\n",
    "    def P(self, word, N=sum(WORDS.values())): \n",
    "        \"Probability of `word`.\"\n",
    "        return WORDS[word] / N\n",
    "\n",
    "    def correction(self, word): \n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words): \n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word): \n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Corrections:\n",
      "             Original Sentence              Norvig Correction            Enhanced Correction\n",
      "                   dking sport                    dking sport                    doing sport\n",
      "            the qick brown fox             the qick brown fox            the quick brown fox\n",
      "       speling corector iz fyn        speling corector iz fyn      spelling corrector is fun\n",
      "             species are dking              species are dking              species are doing\n",
      "           he is a great playr            he is a great playr           he is a great player\n",
      "        my frend is doing well         my frend is doing well        my friend is doing well\n",
      "         what a wondrful wrold          what a wondrful wrold         what a wonderful world\n",
      "             the sun is shning              the sun is shning             the sun is shining\n",
      "         the weeather is nicee          the weeather is nicee            the weather is nice\n",
      "              i loe programing               i loe programing             i love programming\n",
      "                  ths iy a tst                   ths iy a tst                 this is a test\n",
      "                    helto worl                     helto worl                    hello world\n",
      "                spellng is fun                 spellng is fun                spelling is fun\n",
      "\n",
      "Word Corrections:\n",
      "                 Original Word              Norvig Correction            Enhanced Correction\n",
      "                         dking                           king                           king\n",
      "                       speling                       spelling                       spelling\n",
      "                          qick                           pick                           pick\n",
      "                         playr                           play                           play\n",
      "                      wondrful                      wonderful                      wonderful\n",
      "                         frend                         friend                         friend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_sentences = [\n",
    "    \"dking sport\",\n",
    "    \"the qick brown fox\",\n",
    "    \"speling corector iz fyn\",\n",
    "    \"species are dking\",\n",
    "    \"he is a great playr\",\n",
    "    \"my frend is doing well\",\n",
    "    \"what a wondrful wrold\",\n",
    "    \"the sun is shning\",\n",
    "    \"the weeather is nicee\",\n",
    "    \"i loe programing\",\n",
    "    \"ths iy a tst\",\n",
    "    \"helto worl\",\n",
    "    \"spellng is fun\",\n",
    "]\n",
    "\n",
    "test_words = [\n",
    "    \"dking\",\n",
    "    \"speling\",\n",
    "    \"qick\",\n",
    "    \"playr\",\n",
    "    \"wondrful\",\n",
    "    \"frend\",\n",
    "]\n",
    "\n",
    "norvig = NorvigSpellCorrector().correction\n",
    "enhanced_norvig = SpellCorrector().correction\n",
    "\n",
    "# Collect results for sentences\n",
    "sentence_results = []\n",
    "for sentence in test_sentences:\n",
    "    norvig_correction = norvig(sentence)\n",
    "    enhanced_correction = enhanced_norvig(sentence)\n",
    "    sentence_results.append([sentence, norvig_correction, enhanced_correction])\n",
    "\n",
    "# Collect results for words\n",
    "word_results = []\n",
    "for word in test_words:\n",
    "    norvig_correction = norvig(word)\n",
    "    enhanced_correction = enhanced_norvig(word)\n",
    "    word_results.append([word, norvig_correction, enhanced_correction])\n",
    "\n",
    "# Create DataFrames\n",
    "sentence_df = pd.DataFrame(sentence_results, columns=['Original Sentence', 'Norvig Correction', 'Enhanced Correction'])\n",
    "word_df = pd.DataFrame(word_results, columns=['Original Word', 'Norvig Correction', 'Enhanced Correction'])\n",
    "\n",
    "# Display results\n",
    "print(\"Sentence Corrections:\")\n",
    "print(sentence_df.to_string(index=False, col_space=30))\n",
    "\n",
    "print(\"\\nWord Corrections:\")\n",
    "print(word_df.to_string(index=False, col_space=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('unit_tests for <__main__.NorvigSpellCorrector object at 0x000002218D260340> pass',\n",
       " 'unit_tests for <__main__.SpellCorrector object at 0x000002218D260340> pass')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc = NorvigSpellCorrector()\n",
    "\n",
    "def unit_tests(corrector):\n",
    "    assert corrector.correction('speling') == 'spelling'              # insert\n",
    "    assert corrector.correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert corrector.correction('bycycle') == 'bicycle'               # replace\n",
    "    assert corrector.correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert corrector.correction('arrainged') == 'arranged'            # delete\n",
    "    assert corrector.correction('peotry') =='poetry'                  # transpose\n",
    "    assert corrector.correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert corrector.correction('word') == 'word'                     # known\n",
    "    return f'unit_tests for {corrector} pass'\n",
    "\n",
    "unit_tests(NorvigSpellCorrector()), unit_tests(SpellCorrector())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Norvig’s Spell Corrector vs. Context-Sensitive Spell Corrector:\n",
    "\n",
    "| Aspect                         | Norvig’s Spell Corrector                          | Enhanced Spell Corrector                                 |\n",
    "|--------------------------------|---------------------------------------------------|----------------------------------------------------------|\n",
    "| **Context Handling**           | Uses unigram probabilities and candidate generation based on one- and two-edit distances. This works well for isolated words. | Incorporates context through bigram probabilities as well as bidirectional (forward and backward) dynamic programming. It leverages both left- and right-contexts, leading to more informed corrections. |\n",
    "| **Candidate Generation**       | Generates candidates from known words, one-edit, and two-edits; correction is selected based solely on word frequency. | Uses the same candidate generation strategy but weighs candidate scores by combining their unigram probabilities with the likelihoods of word pairs (bigram probabilities) from neighboring context. |\n",
    "| **Sentence Level Correction**  | Applies correction word by word; often leaves multi-word errors unresolved. | Corrects entire sentences using a global optimization (Viterbi algorithm) that considers the sequence of corrections, allowing corrections that depend on surrounding words (e.g., “dking sport” &rarr; “doing sport”). |\n",
    "| **Errors Resolved**            | Performs well on simple misspellings, but may struggle with context-dependent errors. | More likely to select context-appropriate corrections, especially for homophones or common word confusions where the surrounding context is important (e.g., “qick” &rarr; “quick” when surrounded by “the” and “brown fox”). |\n",
    "| **Test Results**               | Norvig’s approach left “dking sport” unchanged and “speling corector iz fyn” unchanged. | The enhanced version corrected “dking sport” to “doing sport” and transformed “speling corector iz fyn” into “spelling corrector is fun”, demonstrating its sensitivity to context. |\n",
    "\n",
    "#### Advantages of the Enhanced Approach:\n",
    "\n",
    "- **Context Sensitivity:**  \n",
    "  By combining forward and backward bigram probabilities, the enhanced model chooses corrections that fit naturally within the sentence context.\n",
    "\n",
    "- **Bidirectional Processing:**  \n",
    "  The use of dynamic programming in both directions ensures the selection of a candidate that is consistent with its immediate neighbors, reducing the risk of isolated correction errors.\n",
    "\n",
    "- **Improved Correction Accuracy:**  \n",
    "  The approach rectifies errors that require context (e.g., “dking species” vs. “dking sport”), thus producing more natural and semantically correct sentence outputs.\n",
    "\n",
    "Overall, while Norvig’s solution provides a solid baseline based on word frequencies and minimal edit distances, the enhanced approach offers significant improvements when dealing with context-dependent errors by taking advantage of both left and right word relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
